<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习—K近邻算法（KNN） | 星海流光</title><meta name="author" content="秋风、萧瑟"><meta name="copyright" content="秋风、萧瑟"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="第一部分：KNN算法详解 (K-近邻算法)1. 核心思想KNN（K-Nearest Neighbors）是一种监督学习算法，既可以用于分类任务，也可以用于回归任务。它的核心思想是：一个样本的类别&#x2F;数值，由其在特征空间中最邻近的 K 个样本的类别&#x2F;数值来决定。 2. 算法步骤 (以分类任务为例)假设我们有一个带标签的训练数据集，现在来了一个新的、没有标签的数据点，我们要预测它的类">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习—K近邻算法（KNN）">
<meta property="og:url" content="https://lx-cel.github.io/2025/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%EF%BC%88KNN%EF%BC%89/index.html">
<meta property="og:site_name" content="星海流光">
<meta property="og:description" content="第一部分：KNN算法详解 (K-近邻算法)1. 核心思想KNN（K-Nearest Neighbors）是一种监督学习算法，既可以用于分类任务，也可以用于回归任务。它的核心思想是：一个样本的类别&#x2F;数值，由其在特征空间中最邻近的 K 个样本的类别&#x2F;数值来决定。 2. 算法步骤 (以分类任务为例)假设我们有一个带标签的训练数据集，现在来了一个新的、没有标签的数据点，我们要预测它的类">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lx-cel.github.io/img/19.png">
<meta property="article:published_time" content="2025-07-05T07:41:14.000Z">
<meta property="article:modified_time" content="2025-07-04T16:00:00.000Z">
<meta property="article:author" content="秋风、萧瑟">
<meta property="article:tag" content="学习笔记">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lx-cel.github.io/img/19.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://lx-cel.github.io/2025/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%EF%BC%88KNN%EF%BC%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习—K近邻算法（KNN）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 8.1.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/Cecilia.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">50</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/19.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">星海流光</span></a><a class="nav-page-title" href="/"><span class="site-name">机器学习—K近邻算法（KNN）</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">机器学习—K近邻算法（KNN）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-05T07:41:14.000Z" title="发表于 2025-07-05 15:41:14">2025-07-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-04T16:00:00.000Z" title="更新于 2025-07-05 00:00:00">2025-07-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h3 id="第一部分：KNN算法详解-K-近邻算法"><a href="#第一部分：KNN算法详解-K-近邻算法" class="headerlink" title="第一部分：KNN算法详解 (K-近邻算法)"></a><strong>第一部分：KNN算法详解 (K-近邻算法)</strong></h3><h4 id="1-核心思想"><a href="#1-核心思想" class="headerlink" title="1. 核心思想"></a><strong>1. 核心思想</strong></h4><p>KNN（K-Nearest Neighbors）是一种<strong>监督学习</strong>算法，既可以用于<strong>分类</strong>任务，也可以用于<strong>回归</strong>任务。它的核心思想是：一个样本的类别&#x2F;数值，由其在特征空间中<strong>最邻近的 K 个样本</strong>的类别&#x2F;数值来决定。</p>
<h4 id="2-算法步骤-以分类任务为例"><a href="#2-算法步骤-以分类任务为例" class="headerlink" title="2. 算法步骤 (以分类任务为例)"></a><strong>2. 算法步骤 (以分类任务为例)</strong></h4><p>假设我们有一个带标签的训练数据集，现在来了一个新的、没有标签的数据点，我们要预测它的类别。</p>
<p><strong>Step 1: 确定超参数 K</strong><br>K是一个正整数，代表我们要参考“邻居”的数量。这个值需要我们预先设定。比如，我们选择 K&#x3D;3。</p>
<p><strong>Step 2: 计算距离</strong><br>计算这个新的数据点与训练集中<strong>每一个</strong>数据点的距离。距离的度量方式有很多种，最常用的是：</p>
<ul>
<li><p><strong>欧氏距离 (Euclidean Distance):</strong><br>$$ d(x, y) &#x3D; \sqrt{\sum_{i&#x3D;1}^{n}(x_i - y_i)^2} $$</p>
</li>
<li><p><strong>曼哈顿距离 (Manhattan Distance):</strong><br>$$ d(x, y) &#x3D; \sum_{i&#x3D;1}^{n}|x_i - y_i| $$</p>
</li>
<li><p><strong>闵可夫斯基距离 (Minkowski Distance):</strong><br>$$ d(x, y) &#x3D; (\sum_{i&#x3D;1}^{n}|x_i - y_i|^p)^{1&#x2F;p} $$</p>
</li>
</ul>
<p><strong>Step 3: 找到最近的 K 个邻居</strong><br>根据上一步计算出的距离，对所有训练样本进行排序，找出距离最近的 K 个样本。</p>
<p><strong>Step 4: 做出决策</strong></p>
<ul>
<li><strong>用于分类：</strong> 在这 K 个邻居中，采用“少数服从多数”的原则，哪个类别的样本最多，新的数据点就被预测为哪个类别。</li>
<li><strong>用于回归：</strong> 预测一个具体的数值。通常是取这 K 个邻居的数值的<strong>平均值</strong>或<strong>加权平均值</strong>（例如，距离越近的邻居权重越高）。</li>
</ul>
<hr>
<h3 id="第二部分：以MNIST数据集为例，说明距离的计算过程"><a href="#第二部分：以MNIST数据集为例，说明距离的计算过程" class="headerlink" title="第二部分：以MNIST数据集为例，说明距离的计算过程"></a><strong>第二部分：以MNIST数据集为例，说明距离的计算过程</strong></h3><h4 id="第一步：理解数据本身——图像如何表示为向量"><a href="#第一步：理解数据本身——图像如何表示为向量" class="headerlink" title="第一步：理解数据本身——图像如何表示为向量"></a><strong>第一步：理解数据本身——图像如何表示为向量</strong></h4><p>MNIST 数据集中的每一张图片都是一个 28x28 像素的灰度图。</p>
<p><img src="/img_1/02.png" alt="MNIST数据集中的&quot;7&quot;"></p>
<ul>
<li><strong>图像：</strong> 在我们眼中，它是一张图片，比如一个手写的 “7”。</li>
<li><strong>计算机的表示：</strong> 对计算机来说，它是一个 28x28 的矩阵。矩阵中的每一个元素代表一个像素点，其值在 0 到 255 之间，表示该点的灰度（0代表纯黑，255代表纯白）。</li>
</ul>
<p>为了在 KNN 算法中计算距离，我们首先需要把这个二维的图像矩阵“拉平”（Flatten），变成一个一维的向量。</p>
<ul>
<li><strong>原始矩阵:</strong> 一个 28x28 的矩阵</li>
<li><strong>拉平操作:</strong> 将矩阵的第一行、第二行、…、第二十八行，依次拼接起来，形成一个长长的一维向量。</li>
<li><strong>特征向量:</strong> 这个向量的维度就是 <code>28 * 28 = 784</code>。所以，<strong>每一张手写数字图片，都被表示为了一个 784 维空间中的一个点</strong>。</li>
</ul>
<p>这个 784 维的向量就是这张图片的<strong>特征向量 (Feature Vector)</strong>。</p>
<h4 id="第二步：计算距离——在784维空间中丈量"><a href="#第二步：计算距离——在784维空间中丈量" class="headerlink" title="第二步：计算距离——在784维空间中丈量"></a><strong>第二步：计算距离——在784维空间中丈量</strong></h4><p>现在，我们有了两个手写数字图片，比如图片A和图片B。它们分别被转换成了两个 784 维的向量：</p>
<ul>
<li>向量 <code>A = (a₁, a₂, a₃, ..., a₇₈₄)</code></li>
<li>向量 <code>B = (b₁, b₂, b₃, ..., b₇₈₄)</code></li>
</ul>
<p>其中 <code>aᵢ</code> 和 <code>bᵢ</code> 分别是两张图片第 <code>i</code> 个像素点的灰度值。</p>
<p>接下来，我们就可以像在二维、三维空间中一样，计算这两个高维向量之间的距离了。最常用的仍然是<strong>欧氏距离</strong>。</p>
<p><strong>欧氏距离计算公式：</strong><br>$$ d(A, B) &#x3D; \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \dots + (a_{784} - b_{784})^2} $$<br>$$ d(A, B) &#x3D; \sqrt{\sum_{i&#x3D;1}^{784}(a_i - b_i)^2} $$</p>
<p><strong>直观理解这个距离的含义：</strong><br>这个距离衡量的是两张图片在<strong>逐个像素点上的差异程度</strong>。</p>
<ul>
<li>如果两张图片非常相似（比如两个不同人写的、但都很标准的 “1”），那么它们对应位置的像素值会很接近，<code>(aᵢ - bᵢ)²</code> 的值会很小，最终计算出的总距离也会很小。</li>
<li>如果两张图片差异巨大（比如一个是 “1”，一个是 “8”），那么它们在很多位置上的像素值都会有很大差别，<code>(aᵢ - bᵢ)²</code> 的值会很大，最终计算出的总距离也就会很大。</li>
</ul>
<p><strong>举一个简化的例子：</strong></p>
<p>假设我们处理的是 3x3 的迷你图片，而不是 28x28。</p>
<p><strong>图片A (一个”X”):</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[255,   0, 255],</span><br><span class="line"> [  0, 255,   0],</span><br><span class="line"> [255,   0, 255]]</span><br></pre></td></tr></table></figure>
<p>拉平后的向量 <code>A = (255, 0, 255, 0, 255, 0, 255, 0, 255)</code></p>
<p><strong>图片B (一个”O”):</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[  0, 255,   0],</span><br><span class="line"> [255,   0, 255],</span><br><span class="line"> [  0, 255,   0]]</span><br></pre></td></tr></table></figure>
<p>拉平后的向量 <code>B = (0, 255, 0, 255, 0, 255, 0, 255, 0)</code></p>
<p><strong>计算A和B的欧氏距离：</strong><br>$$ d(A, B) &#x3D; \sqrt{ (255-0)^2 + (0-255)^2 + (255-0)^2 + \dots } $$<br>你会发现每一项都是 <code>(±255)²</code>，这个距离会非常大。</p>
<p>现在，再来一张<strong>图片C (一个略有不同的”X”)</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[250,   5, 250],</span><br><span class="line"> [ 10, 245,  10],</span><br><span class="line"> [250,   5, 250]]</span><br></pre></td></tr></table></figure>
<p>拉平后的向量 <code>C = (250, 5, 250, 10, 245, 10, 250, 5, 250)</code></p>
<p><strong>计算A和C的欧氏距离：</strong><br>$$ d(A, C) &#x3D; \sqrt{ (255-250)^2 + (0-5)^2 + (255-250)^2 + \dots } $$<br>$$ d(A, C) &#x3D; \sqrt{ 5^2 + (-5)^2 + 5^2 + \dots } $$<br>你会发现每一项的差值都很小，所以 <code>d(A, C)</code> 会远远小于 <code>d(A, B)</code>。</p>
<p>因此，在KNN算法中，如果拿图片C去预测，它很可能会找到图片A作为它的近邻，从而被正确地分类为”X”。</p>
<p><strong>面试中的引申问题：</strong></p>
<ol>
<li><p><strong>数据归一化在这里重要吗？</strong></p>
<ul>
<li><strong>回答：</strong> 是的，很重要。虽然MNIST所有特征（像素值）的量纲都是一样的（0-255），但进行归一化（比如将所有像素值除以255，缩放到[0, 1]区间）是一个非常好的习惯。这样做可以<strong>改善模型的数值稳定性</strong>，并且在后续使用梯度下降等优化算法时（虽然KNN不用，但深度学习用）能<strong>加快收敛速度</strong>。所以，这是一个标准的预处理步骤。</li>
</ul>
</li>
<li><p><strong>784维是不是太高了？这有什么问题吗？</strong></p>
<ul>
<li><strong>回答：</strong> 是的，784维属于高维数据。这会引出我们之前讨论过的**“维度灾难”**问题。在高维空间中，数据点会变得稀疏，距离的区分度可能会下降。尽管如此，KNN在MNIST上依然能取得不错的（超过95%）的准确率，证明了这种基于像素的距离度量在一定程度上是有效的。</li>
<li><strong>进一步加分：</strong> 我们可以通过<strong>主成分分析（PCA）<strong>等降维技术，将784维的特征向量降到更低的维度（比如50维或100维），同时保留大部分信息。在降维后的空间里再使用KNN，通常可以</strong>大幅提升计算速度</strong>，并且有时还能<strong>因为去除了噪声而提升模型性能</strong>。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="第三部分：以MNIST数据集为例，说明KNN的执行步骤"><a href="#第三部分：以MNIST数据集为例，说明KNN的执行步骤" class="headerlink" title="第三部分：以MNIST数据集为例，说明KNN的执行步骤"></a><strong>第三部分：以MNIST数据集为例，说明KNN的执行步骤</strong></h3><p><strong>数据集背景:</strong></p>
<ul>
<li><strong>训练集 (Training Set):</strong> 60,000 张 28x28 像素的图片，每张图片都有一个明确的标签（0, 1, 2, …, 9）。这是我们的“参考答案库”。</li>
<li><strong>测试集 (Test Set):</strong> 10,000 张 28x28 像素的图片，同样带有标签。我们用它来评估模型的效果，但在预测时，我们会假装不知道它的标签。</li>
</ul>
<h4 id="KNN在MNIST上的执行步骤"><a href="#KNN在MNIST上的执行步骤" class="headerlink" title="KNN在MNIST上的执行步骤"></a><strong>KNN在MNIST上的执行步骤</strong></h4><h5 id="预备阶段：数据准备-Data-Preparation"><a href="#预备阶段：数据准备-Data-Preparation" class="headerlink" title="预备阶段：数据准备 (Data Preparation)"></a><strong>预备阶段：数据准备 (Data Preparation)</strong></h5><p>在运行算法之前，我们必须先处理数据。</p>
<ol>
<li><p><strong>数据加载：</strong> 从文件中加载60,000张训练图片、对应的60,000个标签，以及10,000张测试图片和对应的10,000个标签。</p>
</li>
<li><p><strong>数据“拉平”(Flattening)：</strong></p>
<ul>
<li>每一张图片是一个 <code>28x28</code> 的二维矩阵。</li>
<li>为了计算距离，我们将其转换为一个 <code>1x784</code> 的一维向量（特征向量）。</li>
<li>操作后，我们的训练集就变成了一个 <code>60000 x 784</code> 的巨大矩阵，每一行代表一张图片。</li>
</ul>
</li>
<li><p><strong>数据归一化 (Normalization)：</strong></p>
<ul>
<li>原始像素值的范围是 [0, 255]。不同图片中笔画的深浅可能会影响距离计算。</li>
<li>我们将所有像素值都除以 255，将它们缩放到 [0, 1] 区间。这是一个标准的预处理步骤，能提升模型的稳定性和性能。</li>
<li>现在，训练集矩阵里的每个元素都在0和1之间。</li>
</ul>
</li>
</ol>
<h5 id="算法执行阶段-以预测一张新的测试图片为例"><a href="#算法执行阶段-以预测一张新的测试图片为例" class="headerlink" title="算法执行阶段 (以预测一张新的测试图片为例)"></a><strong>算法执行阶段 (以预测一张新的测试图片为例)</strong></h5><p>假设我们从10,000张测试图片中，拿出<strong>第一张</strong>图片，想预测它是什么数字。我们称之为<code>test_image_1</code>。</p>
<p><strong>第1步：确定超参数 K</strong><br>这是我们必须预先决定的。K值的选择会影响最终结果。我们先选择一个常见的、较小的值，比如 <strong>K&#x3D;5</strong>。</p>
<p><strong>第2步：计算距离</strong><br>这是最核心、也是最耗时的一步。</p>
<ul>
<li>拿出 <code>test_image_1</code> 的784维向量。</li>
<li>将这个向量与训练集中<strong>每一张</strong>图片（共60,000张）的784维向量，逐一计算<strong>欧氏距离</strong>。</li>
<li>这个过程会产生 <strong>60,000个</strong> 距离值。每一个距离值都代表了 <code>test_image_1</code> 与某一张训练图片的“相似程度”（距离越小越相似）。</li>
</ul>
<p><strong>第3步：找到最近的 K 个邻居</strong></p>
<ul>
<li>我们有了一个包含60,000个距离值的列表。</li>
<li>对这个列表进行<strong>升序排序</strong>，找到值最小的前 K 个。在我们这个例子里，就是找到最小的 <strong>5个</strong> 距离值。</li>
<li>同时，记录下这5个距离值所对应的<strong>训练图片的原始标签</strong>。</li>
</ul>
<p><strong>第4步：投票决定类别</strong></p>
<ul>
<li>假设我们找到的5个最近邻居，它们的标签分别是：<code>{7, 9, 7, 1, 7}</code>。</li>
<li>我们进行“投票统计”：<ul>
<li>数字 “7” 出现了 3 次。</li>
<li>数字 “9” 出现了 1 次。</li>
<li>数字 “1” 出现了 1 次。</li>
</ul>
</li>
<li>根据“少数服从多数”原则，<strong>数字 “7” 的票数最多</strong>。</li>
</ul>
<p><strong>第5步：做出预测</strong></p>
<ul>
<li>我们的KNN模型最终预测 <code>test_image_1</code> 的类别为 <strong>7</strong>。</li>
</ul>
<h5 id="模型评估阶段-Evaluation"><a href="#模型评估阶段-Evaluation" class="headerlink" title="模型评估阶段 (Evaluation)"></a><strong>模型评估阶段 (Evaluation)</strong></h5><p>预测完一张图片还不够，我们需要知道模型的整体性能。</p>
<p><strong>第6步：评估准确率</strong></p>
<ul>
<li>我们拿出 <code>test_image_1</code> 的<strong>真实标签</strong>。假设它的真实标签确实是 “7”，那么这次预测就是<strong>正确</strong>的。如果真实标签是 “2”，那这次预测就是<strong>错误</strong>的。</li>
<li><strong>重复第2步到第5步</strong>，对测试集中<strong>所有10,000张图片</strong>都进行一次预测。</li>
<li>统计所有预测正确的次数。</li>
<li>计算最终的准确率：<br>$$ \text{Accuracy} &#x3D; \frac{\text{Number of Correct Predictions}}{\text{Total Number of Test Images}} &#x3D; \frac{\text{预测正确的图片数}}{10000} $$</li>
</ul>
<p>例如，如果我们正确预测了9,680张图片，那么模型的准确率就是 <code>9680 / 10000 = 96.8%</code>。</p>
<hr>
<h3 id="第四部分：核心考察点"><a href="#第四部分：核心考察点" class="headerlink" title="第四部分：核心考察点"></a><strong>第四部分：核心考察点</strong></h3><h4 id="考点1：KNN的优缺点是什么？"><a href="#考点1：KNN的优缺点是什么？" class="headerlink" title="考点1：KNN的优缺点是什么？"></a><strong>考点1：KNN的优缺点是什么？</strong></h4><p><strong>优点 (Pros):</strong></p>
<ol>
<li><strong>简单直观：</strong> 算法思想简单，易于理解和实现。是很好的入门和基线模型（Baseline）。</li>
<li><strong>非参数模型 (Non-parametric):</strong> 对数据分布没有假设（比如不像线性回归那样假设数据是线性的）。这使得KNN在处理一些非线性、复杂边界问题时有奇效。</li>
<li><strong>无需训练 (Lazy Learning):</strong> KNN是“懒惰学习”的代表。它没有显式的训练过程，只是把训练数据存储起来。训练时间开销为零。</li>
</ol>
<p><strong>缺点 (Cons):</strong></p>
<ol>
<li><strong>计算成本高昂：</strong> 预测阶段的计算量巨大。对于一个新样本，需要和所有N个训练样本计算距离，时间复杂度为O(N*d)，其中d是特征维度。当数据集很大时，预测会非常慢。</li>
<li><strong>内存占用大：</strong> 需要存储全部训练数据，对内存消耗大。</li>
<li><strong>对K值敏感：</strong> K值的选择直接影响模型性能，需要通过交叉验证等方法小心选择。</li>
<li><strong>受样本不均衡问题影响大：</strong> 如果某些类别的样本数量远多于其他类别，那么在投票时，数量多的类别会占据天然优势。</li>
<li><strong>维度灾难 (Curse of Dimensionality)：</strong> 随着特征维度的增加，KNN的性能会急剧下降。</li>
</ol>
<h4 id="考点2：K值的选择与影响-Bias-Variance-Tradeoff"><a href="#考点2：K值的选择与影响-Bias-Variance-Tradeoff" class="headerlink" title="考点2：K值的选择与影响 (Bias-Variance Tradeoff)"></a><strong>考点2：K值的选择与影响 (Bias-Variance Tradeoff)</strong></h4><ul>
<li><p><strong>较小的 K 值：</strong></p>
<ul>
<li>模型会变得更复杂，决策边界会更不规则。</li>
<li>容易受到噪声数据的影响，导致<strong>过拟合 (Overfitting)</strong>。</li>
<li>表现为<strong>低偏差 (Low Bias)</strong> 和 <strong>高方差 (High Variance)</strong>。</li>
</ul>
</li>
<li><p><strong>较大的 K 值：</strong></p>
<ul>
<li>模型会变得更简单，决策边界会更平滑。</li>
<li>会忽略数据中局部的、细微的结构，导致<strong>欠拟合 (Underfitting)</strong>。</li>
<li>表现为<strong>高偏差 (High Bias)</strong> 和 <strong>低方差 (Low Variance)</strong>。</li>
</ul>
</li>
<li><p><strong>如何选择K值？</strong></p>
<ul>
<li><strong>交叉验证 (Cross-Validation):</strong> 最常用的方法。将训练集分成多份，轮流作为验证集来测试不同K值的效果，选择在验证集上平均表现最好的K值。</li>
<li>通常K值会选择一个<strong>奇数</strong>，以避免在二分类问题中出现票数相等的情况。</li>
</ul>
</li>
</ul>
<h4 id="考点3：为什么KNN需要对数据进行归一化-Normalization-Standardization-？"><a href="#考点3：为什么KNN需要对数据进行归一化-Normalization-Standardization-？" class="headerlink" title="考点3：为什么KNN需要对数据进行归一化(Normalization&#x2F;Standardization)？"></a><strong>考点3：为什么KNN需要对数据进行归一化(Normalization&#x2F;Standardization)？</strong></h4><ul>
<li><strong>核心原因：</strong> KNN是基于<strong>距离</strong>的算法。如果不同特征的量纲（尺度）差异巨大，那么距离的计算将完全由尺度大的特征主导。</li>
<li><strong>举例说明：</strong> 假设有两个特征：身高（单位：cm，范围150-200）和体重（单位：kg，范围40-100）。在计算欧氏距离时，身高的差值平方项会比体重的差值平方项大得多，这会使得模型在计算距离时，几乎只考虑了身高，而忽略了体重这个特征。这显然是不合理的。</li>
<li><strong>解决方案：</strong><ul>
<li><strong>归一化 (Normalization):</strong> 将数据缩放到 [0, 1] 区间。公式：<code>X_norm = (X - X_min) / (X_max - X_min)</code></li>
<li><strong>标准化 (Standardization):</strong> 将数据缩放到均值为0，标准差为1的分布。公式：<code>X_std = (X - mean(X)) / std(X)</code></li>
<li>面试时能说出这个原因并给出例子，就是满分回答。</li>
</ul>
</li>
</ul>
<h4 id="考点4：如何理解KNN中的“维度灾难”？"><a href="#考点4：如何理解KNN中的“维度灾难”？" class="headerlink" title="考点4：如何理解KNN中的“维度灾难”？"></a><strong>考点4：如何理解KNN中的“维度灾难”？</strong></h4><ul>
<li><strong>直观理解：</strong> 在高维空间中，数据点会变得非常<strong>稀疏</strong>。想象一下，在一个一维线段上撒10个点很容易密集，但在一个三维立方体里撒10个点，它们会非常分散。</li>
<li><strong>距离失效：</strong> 在高维空间中，任意两点之间的欧氏距离的差值会变得很小，趋向于一致。换句话说，对于一个给定的查询点，它到“最近”邻居的距离和到“最远”邻居的距离相差无几。这使得“近邻”这个概念失去了意义。</li>
<li><strong>如何缓解？</strong><ul>
<li><strong>特征选择&#x2F;降维：</strong> 使用PCA等方法对数据进行降维，去除冗余和不重要的特征。</li>
</ul>
</li>
</ul>
<h4 id="考点5：如何提高KNN的效率？"><a href="#考点5：如何提高KNN的效率？" class="headerlink" title="考点5：如何提高KNN的效率？"></a><strong>考点5：如何提高KNN的效率？</strong></h4><ul>
<li><strong>问题根源：</strong> KNN的瓶颈在于预测时需要进行大量的距离计算（暴力搜索）。</li>
<li><strong>解决方案：</strong> 使用<strong>空间索引数据结构</strong>来加速近邻搜索，避免全局扫描。<ul>
<li><strong>KD-Tree (K-Dimensional Tree):</strong> 这是一种对K维空间中的点进行划分的数据结构。它通过在不同维度上轮流切分数据空间，构建一个二叉树。在搜索时，可以快速剪枝掉那些不可能包含最近邻居的子空间，从而大大减少距离计算的次数。它在<strong>低维</strong>（例如d &lt; 20）时效率很高。</li>
<li><strong>Ball-Tree:</strong> 当维度非常高时，KD-Tree的效率会下降。Ball-Tree通过将数据点划分在一系列嵌套的超球体（Hypersphere）中来构建树。它对高维数据更具鲁棒性。</li>
</ul>
</li>
</ul>
<h4 id="考点6：KNN-和-K-Means-有什么区别？"><a href="#考点6：KNN-和-K-Means-有什么区别？" class="headerlink" title="考点6：KNN 和 K-Means 有什么区别？"></a><strong>考点6：KNN 和 K-Means 有什么区别？</strong></h4><table>
<thead>
<tr>
<th align="left">特性</th>
<th align="left">KNN (K-近邻)</th>
<th align="left">K-Means (K-均值)</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>算法类别</strong></td>
<td align="left"><strong>监督学习</strong> (Supervised Learning)</td>
<td align="left"><strong>无监督学习</strong> (Unsupervised Learning)</td>
</tr>
<tr>
<td align="left"><strong>解决问题</strong></td>
<td align="left"><strong>分类</strong> 或 <strong>回归</strong></td>
<td align="left"><strong>聚类</strong> (Clustering)</td>
</tr>
<tr>
<td align="left"><strong>数据要求</strong></td>
<td align="left">需要带<strong>标签</strong>的训练数据</td>
<td align="left">只需要数据点，<strong>无需标签</strong></td>
</tr>
<tr>
<td align="left"><strong>核心思想</strong></td>
<td align="left">基于邻居投票&#x2F;平均来预测新样本</td>
<td align="left">找到K个簇中心，使数据点到其所属簇中心的距离之和最小</td>
</tr>
<tr>
<td align="left"><strong>“K”的含义</strong></td>
<td align="left">邻居的数量（超参数）</td>
<td align="left">簇的数量（超参数）</td>
</tr>
</tbody></table>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://lx-cel.github.io">秋风、萧瑟</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://lx-cel.github.io/2025/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%EF%BC%88KNN%EF%BC%89/">https://lx-cel.github.io/2025/07/05/机器学习—K近邻算法（KNN）/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://lx-cel.github.io" target="_blank">星海流光</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post-share"><div class="social-share" data-image="/img/19.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/07/14/Python%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E5%A4%84%E7%90%86/" title="Python输入输出处理"><img class="cover" src="/img/20.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Python输入输出处理</div></div><div class="info-2"><div class="info-item-1">核心知识点 input() vs sys.stdin.readline()  input(): 内置函数，使用方便，但速度较慢。它会读取一行，去除末尾的换行符 \n，并返回一个字符串。 sys.stdin.readline(): 速度更快，因为它使用了缓冲区。它会读取一行，但保留末尾的换行符 \n。因此，通常需要配合 .strip() 或 .rstrip() 使用。   print() vs sys.stdout.write()  print(): 内置函数，功能强大，可以自动在末尾添加换行符，但相对较慢。 sys.stdout.write(): 速度更快，但只接受字符串作为参数，且不会自动添加换行符，需要手动添加 &#39;\n&#39;。    对于追求极致性能的竞赛，推荐使用 sys 模块。 12import sys# 之后就可以使用 sys.stdin.readline() 和 sys.stdout.write()   一、 输入 (Input)场景1：读取单行数据1.1 读取一个字符串12345678910# 方法一：使用 input() (方便，但稍慢)s =...</div></div></div></a><a class="pagination-related" href="/2025/07/02/Python%E4%B8%AD%E7%9A%84sort%E6%96%B9%E6%B3%95%E3%80%81sorted%E5%87%BD%E6%95%B0%E4%B8%8Elambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/" title="Python中的sort方法、sorted函数与lambda表达式"><img class="cover" src="/img/18.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Python中的sort方法、sorted函数与lambda表达式</div></div><div class="info-2"><div class="info-item-1">1. sort()方法1.1 sort()方法list.sort()最核心、也最需要记住的特点是：它会直接修改原始列表，使其变为有序状态。也就是原地排序。因此，sort()方法的返回值是None。部分初学者可能会将其结果赋值给新变量，这种用法是错误的。正确的用法是无需将其结果赋值给新变量，直接使用被修改后的原列表即可，如下所示： 123nums = [3, 1, 4, 1, 5, 9, 2]nums.sort()print(nums) 结果如下： 1[1, 1, 2, 3, 4, 5, 9] 1.2 基本语法和参数sort()方法的完整语法是： 1list.sort(*, key=None, reverse=False) 它接受两个可自选的关键字参数：key和reverse。 A. reverse参数这个参数非常直观  reverse &#x3D; False（默认值）：升序排序 reverse &#x3D; True：降序排序  123456numbers = [4, 2, 8, 1, 6]numbers.sort() # 默认升序print(numbers) # 输出:...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/10/22/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94std-function%E7%9A%84%E7%94%A8%E6%B3%95/" title="CPP学习笔记—std::function的用法"><img class="cover" src="/img/50.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-22</div><div class="info-item-2">CPP学习笔记—std::function的用法</div></div><div class="info-2"><div class="info-item-1">std::function 是 C++11 中引入的一个极其有用的工具，位于 &lt;functional&gt; 头文件中。它是一个通用的、多态的函数包装器。它的实例可以存储、复制和调用任何可调用 (Callable) 目标——包括普通函数、Lambda 表达式、函数指针、成员函数指针、以及函数对象（functors）。  1. std::function 是什么？一句话定义std::function 是一个类型安全的包装器，它可以持有任何符合其函数签名的可调用对象。 可以把它想象成一个“万能的函数指针”，但它比函数指针强大得多，因为它可以指向任何可调用的东西，而不仅仅是全局函数。 解决的问题：类型不统一的“可调用物”在 C++ 中，有很多东西都可以被“调用”，比如：  普通函数指针: void (*p_func)(int); 函数对象 (Functor): 一个重载了 operator() 的类的对象。每个函数对象都有自己独特的类型。 Lambda 表达式: 编译器会为每个 Lambda 生成一个唯一的、匿名的闭包类型。  在 std::function...</div></div></div></a><a class="pagination-related" href="/2025/09/19/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94priority-queue/" title="CPP学习笔记—priority_queue"><img class="cover" src="/img/38.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-19</div><div class="info-item-2">CPP学习笔记—priority_queue</div></div><div class="info-2"><div class="info-item-1">1. priority_queue 是什么？std::priority_queue 是 C++ 标准模板库（STL）中的一个容器适配器，它提供了一种特殊的队列：优先级队列。 核心概念：优先出队与普通的队列（std::queue）遵循“先进先出”（FIFO）的原则不同，优先级队列中的元素并非根据其进入队列的顺序出队，而是根据其优先级。每次从队列中取出的元素（通过 top() 访问，pop() 移除），都是当前队列中优先级最高的那个。 默认情况下，对于数字，“大”的元素优先级更高；对于其他类型，使用 std::less 作为比较函数，即通过 operator&lt; 来判断优先级。 底层数据结构：堆 (Heap)priority_queue 的高效实现得益于其底层的堆数据结构。通常是最大堆 (Max-Heap)。  堆：一个特殊的完全二叉树，满足“堆属性”：父节点的值总是大于或等于（最大堆）或小于或等于（最小堆）其所有子节点的值。 最大堆 (Max-Heap)：根节点是整个堆中最大的元素。std::priority_queue 默认就是最大堆。 最小堆...</div></div></div></a><a class="pagination-related" href="/2025/09/19/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94unordered-map/" title="CPP学习笔记—unordered_map"><img class="cover" src="/img/37.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-19</div><div class="info-item-2">CPP学习笔记—unordered_map</div></div><div class="info-2"><div class="info-item-1">1. unordered_map 是什么？std::unordered_map 是 C++ 标准模板库（STL）中的一个关联容器，它存储了由“键（Key）”和“值（Value）”组成的元素对（std::pair&lt;const Key, Value&gt;）。它的核心特点是：  无序性：元素在容器内的存储顺序是无序的，遍历 unordered_map 时，得到的元素顺序与插入顺序无关，并且在不同编译环境下可能不同。 基于哈希表：其内部实现是一个哈希表（Hash Table）。 高效查询：由于使用了哈希表，它提供了非常快的平均时间复杂度的操作： 插入、删除、查找：平均 O(1) 最坏情况：O(N)，其中 N 是容器中元素的数量（当发生严重的哈希冲突时）。    核心概念：哈希表为了理解 unordered_map，需要了解哈希表的工作原理：  哈希函数 (Hash Function)：当插入一个键值对时，unordered_map 会使用一个哈希函数将“键”转换成一个整数，这个整数被称为哈希值（Hash Code）。 桶 (Bucket)：unordered_map...</div></div></div></a><a class="pagination-related" href="/2025/10/09/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/" title="CPP学习笔记—单例模式"><img class="cover" src="/img/40.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-09</div><div class="info-item-2">CPP学习笔记—单例模式</div></div><div class="info-2"><div class="info-item-1">1. 什么是单例模式？单例模式是一种创建型设计模式，其核心思想是确保一个类在任何情况下都只有一个实例，并提供一个全局访问点来获取这个唯一的实例。 可以把它想象成一个国家的总统或者一个学校的校长，在整个系统运行期间，这个角色只能有一个人担任。无论你从哪个部门、哪个流程去“找校长”，最终找到的都是同一个人。 为了在 C++ 中实现这一点，通常需要满足三个关键条件：  私有的构造函数：为了防止外部代码通过 new 操作符随意创建类的实例。 一个私有的、静态的、指向本类实例的指针或对象：这是存放那个唯一实例的地方。 一个公有的、静态的、用于获取实例的方法：这是全局唯一的访问点，负责创建并返回那个唯一的实例。   2. 为什么需要单例模式？单例模式主要用于解决那些“全局唯一”且需要被频繁共享访问的资源或服务。常见的应用场景包括：  日志记录器（Logger）：整个应用程序通常只需要一个日志记录器，所有模块都通过它来写入日志文件。 配置管理器（Configuration Manager）：读取和管理应用的配置信息（如数据库连接字符串、API...</div></div></div></a><a class="pagination-related" href="/2025/10/22/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E5%8F%B3%E5%80%BC%E5%BC%95%E7%94%A8%E5%92%8C%E7%A7%BB%E5%8A%A8%E8%AF%AD%E4%B9%89/" title="CPP学习笔记—右值引用和移动语义"><img class="cover" src="/img/49.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-22</div><div class="info-item-2">CPP学习笔记—右值引用和移动语义</div></div><div class="info-2"><div class="info-item-1">右值引用（Rvalue Reference）和移动语义（Move Semantics）是 C++11 中引入的最重要的特性之一，它极大地提升了 C++ 的性能，并使得一些新的编程范式（如资源所有权的唯一性）成为可能。  1. 背景知识：左值（Lvalue）与右值（Rvalue）在 C++ 中，每一个表达式都有两个属性：类型（Type） 和 值类别（Value Category）。值类别中最基本的就是左值和右值。 左值 (Lvalue - Locator Value)可以把它粗略地理解为 “有固定内存地址、可以被赋值” 的表达式。它就像一个有名字、有固定住址的“居民”。  特征： 可以取地址（使用 &amp; 运算符）。 通常出现在赋值运算符 = 的左边。 在表达式结束后依然存在。   例子： 变量名：int x = 10; (x 是一个左值)。 数组元素：arr[0]。 解引用的指针：*p。 返回左值引用的函数调用：get_string_ref()。    右值 (Rvalue - Read Value)可以把它粗略地理解为 “临时的、即将被销毁”...</div></div></div></a><a class="pagination-related" href="/2025/10/03/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E5%88%9D%E5%A7%8B%E5%8C%96%E6%96%B9%E5%BC%8F/" title="CPP学习笔记—初始化方式"><img class="cover" src="/img/39.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-03</div><div class="info-item-2">CPP学习笔记—初始化方式</div></div><div class="info-2"><div class="info-item-1">C++ 的初始化是一个庞大而精细的主题，从 C 语言的简单赋值风格，到 C++11 引入的统一初始化，其发展历程旨在解决二义性、提高类型安全性和代码一致性。 为什么初始化如此重要？在 C++ 中，一个未经初始化的变量（非静态局部变量）拥有一个不确定的值。读取这个值会导致未定义行为 (Undefined Behavior, UB)，这是 C++ 中最危险的陷阱之一，可能导致程序崩溃、数据损坏或看似正常运行但结果错误。  C++ 初始化方式的演变与分类我们可以大致将初始化方式分为两大类：C++11 之前的传统方式和 C++11 及其之后引入的统一初始化（大括号初始化）。 第一部分：C++11 之前的传统初始化方式在 C++11 之前，主要有以下几种初始化方式，它们的语法不统一，有时会带来困惑。 1. 默认初始化 (Default Initialization)当一个变量在定义时没有提供显式的初始值时，就会发生默认初始化。 语法： 1T object_name;  行为：  对于局部变量（在函数内定义）：如果 T 是内置类型（如 int, double,...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/Cecilia.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">秋风、萧瑟</div><div class="author-info-description">梦想是用技术创造一个美好的世界</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">50</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">本站于2024年12月25日正式上线,博客内容主要是记录个人学习和技术分享,也欢迎技术交流!!!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%9AKNN%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3-K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95"><span class="toc-text">第一部分：KNN算法详解 (K-近邻算法)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-text">1. 核心思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4-%E4%BB%A5%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E4%B8%BA%E4%BE%8B"><span class="toc-text">2. 算法步骤 (以分类任务为例)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%9A%E4%BB%A5MNIST%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%BA%E4%BE%8B%EF%BC%8C%E8%AF%B4%E6%98%8E%E8%B7%9D%E7%A6%BB%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-text">第二部分：以MNIST数据集为例，说明距离的计算过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E7%90%86%E8%A7%A3%E6%95%B0%E6%8D%AE%E6%9C%AC%E8%BA%AB%E2%80%94%E2%80%94%E5%9B%BE%E5%83%8F%E5%A6%82%E4%BD%95%E8%A1%A8%E7%A4%BA%E4%B8%BA%E5%90%91%E9%87%8F"><span class="toc-text">第一步：理解数据本身——图像如何表示为向量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E8%AE%A1%E7%AE%97%E8%B7%9D%E7%A6%BB%E2%80%94%E2%80%94%E5%9C%A8784%E7%BB%B4%E7%A9%BA%E9%97%B4%E4%B8%AD%E4%B8%88%E9%87%8F"><span class="toc-text">第二步：计算距离——在784维空间中丈量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%EF%BC%9A%E4%BB%A5MNIST%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%BA%E4%BE%8B%EF%BC%8C%E8%AF%B4%E6%98%8EKNN%E7%9A%84%E6%89%A7%E8%A1%8C%E6%AD%A5%E9%AA%A4"><span class="toc-text">第三部分：以MNIST数据集为例，说明KNN的执行步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#KNN%E5%9C%A8MNIST%E4%B8%8A%E7%9A%84%E6%89%A7%E8%A1%8C%E6%AD%A5%E9%AA%A4"><span class="toc-text">KNN在MNIST上的执行步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%A2%84%E5%A4%87%E9%98%B6%E6%AE%B5%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87-Data-Preparation"><span class="toc-text">预备阶段：数据准备 (Data Preparation)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%89%A7%E8%A1%8C%E9%98%B6%E6%AE%B5-%E4%BB%A5%E9%A2%84%E6%B5%8B%E4%B8%80%E5%BC%A0%E6%96%B0%E7%9A%84%E6%B5%8B%E8%AF%95%E5%9B%BE%E7%89%87%E4%B8%BA%E4%BE%8B"><span class="toc-text">算法执行阶段 (以预测一张新的测试图片为例)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E9%98%B6%E6%AE%B5-Evaluation"><span class="toc-text">模型评估阶段 (Evaluation)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%EF%BC%9A%E6%A0%B8%E5%BF%83%E8%80%83%E5%AF%9F%E7%82%B9"><span class="toc-text">第四部分：核心考察点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%80%83%E7%82%B91%EF%BC%9AKNN%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">考点1：KNN的优缺点是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%80%83%E7%82%B92%EF%BC%9AK%E5%80%BC%E7%9A%84%E9%80%89%E6%8B%A9%E4%B8%8E%E5%BD%B1%E5%93%8D-Bias-Variance-Tradeoff"><span class="toc-text">考点2：K值的选择与影响 (Bias-Variance Tradeoff)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%80%83%E7%82%B93%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88KNN%E9%9C%80%E8%A6%81%E5%AF%B9%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%BD%92%E4%B8%80%E5%8C%96-Normalization-Standardization-%EF%BC%9F"><span class="toc-text">考点3：为什么KNN需要对数据进行归一化(Normalization&#x2F;Standardization)？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%80%83%E7%82%B94%EF%BC%9A%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3KNN%E4%B8%AD%E7%9A%84%E2%80%9C%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE%E2%80%9D%EF%BC%9F"><span class="toc-text">考点4：如何理解KNN中的“维度灾难”？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%80%83%E7%82%B95%EF%BC%9A%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98KNN%E7%9A%84%E6%95%88%E7%8E%87%EF%BC%9F"><span class="toc-text">考点5：如何提高KNN的效率？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%80%83%E7%82%B96%EF%BC%9AKNN-%E5%92%8C-K-Means-%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-text">考点6：KNN 和 K-Means 有什么区别？</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/10/24/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E7%BB%93%E6%9E%84%E5%8C%96%E7%BB%91%E5%AE%9A/" title="CPP学习笔记—结构化绑定"><img src="/img/52.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CPP学习笔记—结构化绑定"/></a><div class="content"><a class="title" href="/2025/10/24/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E7%BB%93%E6%9E%84%E5%8C%96%E7%BB%91%E5%AE%9A/" title="CPP学习笔记—结构化绑定">CPP学习笔记—结构化绑定</a><time datetime="2025-10-24T07:52:23.000Z" title="发表于 2025-10-24 15:52:23">2025-10-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/22/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/" title="CPP学习笔记—策略模式"><img src="/img/51.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CPP学习笔记—策略模式"/></a><div class="content"><a class="title" href="/2025/10/22/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/" title="CPP学习笔记—策略模式">CPP学习笔记—策略模式</a><time datetime="2025-10-22T15:37:59.000Z" title="发表于 2025-10-22 23:37:59">2025-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/22/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94std-function%E7%9A%84%E7%94%A8%E6%B3%95/" title="CPP学习笔记—std::function的用法"><img src="/img/50.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CPP学习笔记—std::function的用法"/></a><div class="content"><a class="title" href="/2025/10/22/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94std-function%E7%9A%84%E7%94%A8%E6%B3%95/" title="CPP学习笔记—std::function的用法">CPP学习笔记—std::function的用法</a><time datetime="2025-10-22T15:33:26.000Z" title="发表于 2025-10-22 23:33:26">2025-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/22/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E5%8F%B3%E5%80%BC%E5%BC%95%E7%94%A8%E5%92%8C%E7%A7%BB%E5%8A%A8%E8%AF%AD%E4%B9%89/" title="CPP学习笔记—右值引用和移动语义"><img src="/img/49.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CPP学习笔记—右值引用和移动语义"/></a><div class="content"><a class="title" href="/2025/10/22/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E5%8F%B3%E5%80%BC%E5%BC%95%E7%94%A8%E5%92%8C%E7%A7%BB%E5%8A%A8%E8%AF%AD%E4%B9%89/" title="CPP学习笔记—右值引用和移动语义">CPP学习笔记—右值引用和移动语义</a><time datetime="2025-10-22T15:26:41.000Z" title="发表于 2025-10-22 23:26:41">2025-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/22/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E7%8E%B0%E4%BB%A3C-%E7%9A%84%E7%89%B9%E6%80%A7/" title="CPP学习笔记—现代C++的特性"><img src="/img/48.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CPP学习笔记—现代C++的特性"/></a><div class="content"><a class="title" href="/2025/10/22/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E7%8E%B0%E4%BB%A3C-%E7%9A%84%E7%89%B9%E6%80%A7/" title="CPP学习笔记—现代C++的特性">CPP学习笔记—现代C++的特性</a><time datetime="2025-10-22T14:43:46.000Z" title="发表于 2025-10-22 22:43:46">2025-10-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By 秋风、萧瑟</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }
      
      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.isShuoshuo
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'LX-Cel/lx-cel.github.io',
      'data-repo-id': 'R_kgDONd3anw',
      'data-category-id': 'DIC_kwDONd3an84ClwRS',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>