<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习—K近邻算法（KNN） | 星海流光</title><meta name="author" content="秋风、萧瑟"><meta name="copyright" content="秋风、萧瑟"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="第一部分：KNN算法详解 (K-近邻算法)1. 核心思想KNN（K-Nearest Neighbors）是一种监督学习算法，既可以用于分类任务，也可以用于回归任务。它的核心思想是：一个样本的类别&#x2F;数值，由其在特征空间中最邻近的 K 个样本的类别&#x2F;数值来决定。 2. 算法步骤 (以分类任务为例)假设我们有一个带标签的训练数据集，现在来了一个新的、没有标签的数据点，我们要预测它的类">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习—K近邻算法（KNN）">
<meta property="og:url" content="https://lx-cel.github.io/2025/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%EF%BC%88KNN%EF%BC%89/index.html">
<meta property="og:site_name" content="星海流光">
<meta property="og:description" content="第一部分：KNN算法详解 (K-近邻算法)1. 核心思想KNN（K-Nearest Neighbors）是一种监督学习算法，既可以用于分类任务，也可以用于回归任务。它的核心思想是：一个样本的类别&#x2F;数值，由其在特征空间中最邻近的 K 个样本的类别&#x2F;数值来决定。 2. 算法步骤 (以分类任务为例)假设我们有一个带标签的训练数据集，现在来了一个新的、没有标签的数据点，我们要预测它的类">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lx-cel.github.io/img/19.png">
<meta property="article:published_time" content="2025-07-05T07:41:14.000Z">
<meta property="article:modified_time" content="2025-07-04T16:00:00.000Z">
<meta property="article:author" content="秋风、萧瑟">
<meta property="article:tag" content="学习笔记">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lx-cel.github.io/img/19.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://lx-cel.github.io/2025/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%EF%BC%88KNN%EF%BC%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习—K近邻算法（KNN）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/Cecilia.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">35</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/19.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">星海流光</span></a><a class="nav-page-title" href="/"><span class="site-name">机器学习—K近邻算法（KNN）</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">机器学习—K近邻算法（KNN）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-05T07:41:14.000Z" title="发表于 2025-07-05 15:41:14">2025-07-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-04T16:00:00.000Z" title="更新于 2025-07-05 00:00:00">2025-07-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h3 id="第一部分：KNN算法详解-K-近邻算法"><a href="#第一部分：KNN算法详解-K-近邻算法" class="headerlink" title="第一部分：KNN算法详解 (K-近邻算法)"></a><strong>第一部分：KNN算法详解 (K-近邻算法)</strong></h3><h4 id="1-核心思想"><a href="#1-核心思想" class="headerlink" title="1. 核心思想"></a><strong>1. 核心思想</strong></h4><p>KNN（K-Nearest Neighbors）是一种<strong>监督学习</strong>算法，既可以用于<strong>分类</strong>任务，也可以用于<strong>回归</strong>任务。它的核心思想是：一个样本的类别&#x2F;数值，由其在特征空间中<strong>最邻近的 K 个样本</strong>的类别&#x2F;数值来决定。</p>
<h4 id="2-算法步骤-以分类任务为例"><a href="#2-算法步骤-以分类任务为例" class="headerlink" title="2. 算法步骤 (以分类任务为例)"></a><strong>2. 算法步骤 (以分类任务为例)</strong></h4><p>假设我们有一个带标签的训练数据集，现在来了一个新的、没有标签的数据点，我们要预测它的类别。</p>
<p><strong>Step 1: 确定超参数 K</strong><br>K是一个正整数，代表我们要参考“邻居”的数量。这个值需要我们预先设定。比如，我们选择 K&#x3D;3。</p>
<p><strong>Step 2: 计算距离</strong><br>计算这个新的数据点与训练集中<strong>每一个</strong>数据点的距离。距离的度量方式有很多种，最常用的是：</p>
<ul>
<li><p><strong>欧氏距离 (Euclidean Distance):</strong><br>$$ d(x, y) &#x3D; \sqrt{\sum_{i&#x3D;1}^{n}(x_i - y_i)^2} $$</p>
</li>
<li><p><strong>曼哈顿距离 (Manhattan Distance):</strong><br>$$ d(x, y) &#x3D; \sum_{i&#x3D;1}^{n}|x_i - y_i| $$</p>
</li>
<li><p><strong>闵可夫斯基距离 (Minkowski Distance):</strong><br>$$ d(x, y) &#x3D; (\sum_{i&#x3D;1}^{n}|x_i - y_i|^p)^{1&#x2F;p} $$</p>
</li>
</ul>
<p><strong>Step 3: 找到最近的 K 个邻居</strong><br>根据上一步计算出的距离，对所有训练样本进行排序，找出距离最近的 K 个样本。</p>
<p><strong>Step 4: 做出决策</strong></p>
<ul>
<li><strong>用于分类：</strong> 在这 K 个邻居中，采用“少数服从多数”的原则，哪个类别的样本最多，新的数据点就被预测为哪个类别。</li>
<li><strong>用于回归：</strong> 预测一个具体的数值。通常是取这 K 个邻居的数值的<strong>平均值</strong>或<strong>加权平均值</strong>（例如，距离越近的邻居权重越高）。</li>
</ul>
<hr>
<h3 id="第二部分：以MNIST数据集为例，说明距离的计算过程"><a href="#第二部分：以MNIST数据集为例，说明距离的计算过程" class="headerlink" title="第二部分：以MNIST数据集为例，说明距离的计算过程"></a><strong>第二部分：以MNIST数据集为例，说明距离的计算过程</strong></h3><h4 id="第一步：理解数据本身——图像如何表示为向量"><a href="#第一步：理解数据本身——图像如何表示为向量" class="headerlink" title="第一步：理解数据本身——图像如何表示为向量"></a><strong>第一步：理解数据本身——图像如何表示为向量</strong></h4><p>MNIST 数据集中的每一张图片都是一个 28x28 像素的灰度图。</p>
<p><img src="/img_1/02.png" alt="MNIST数据集中的&quot;7&quot;"></p>
<ul>
<li><strong>图像：</strong> 在我们眼中，它是一张图片，比如一个手写的 “7”。</li>
<li><strong>计算机的表示：</strong> 对计算机来说，它是一个 28x28 的矩阵。矩阵中的每一个元素代表一个像素点，其值在 0 到 255 之间，表示该点的灰度（0代表纯黑，255代表纯白）。</li>
</ul>
<p>为了在 KNN 算法中计算距离，我们首先需要把这个二维的图像矩阵“拉平”（Flatten），变成一个一维的向量。</p>
<ul>
<li><strong>原始矩阵:</strong> 一个 28x28 的矩阵</li>
<li><strong>拉平操作:</strong> 将矩阵的第一行、第二行、…、第二十八行，依次拼接起来，形成一个长长的一维向量。</li>
<li><strong>特征向量:</strong> 这个向量的维度就是 <code>28 * 28 = 784</code>。所以，<strong>每一张手写数字图片，都被表示为了一个 784 维空间中的一个点</strong>。</li>
</ul>
<p>这个 784 维的向量就是这张图片的<strong>特征向量 (Feature Vector)</strong>。</p>
<h4 id="第二步：计算距离——在784维空间中丈量"><a href="#第二步：计算距离——在784维空间中丈量" class="headerlink" title="第二步：计算距离——在784维空间中丈量"></a><strong>第二步：计算距离——在784维空间中丈量</strong></h4><p>现在，我们有了两个手写数字图片，比如图片A和图片B。它们分别被转换成了两个 784 维的向量：</p>
<ul>
<li>向量 <code>A = (a₁, a₂, a₃, ..., a₇₈₄)</code></li>
<li>向量 <code>B = (b₁, b₂, b₃, ..., b₇₈₄)</code></li>
</ul>
<p>其中 <code>aᵢ</code> 和 <code>bᵢ</code> 分别是两张图片第 <code>i</code> 个像素点的灰度值。</p>
<p>接下来，我们就可以像在二维、三维空间中一样，计算这两个高维向量之间的距离了。最常用的仍然是<strong>欧氏距离</strong>。</p>
<p><strong>欧氏距离计算公式：</strong><br>$$ d(A, B) &#x3D; \sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \dots + (a_{784} - b_{784})^2} $$<br>$$ d(A, B) &#x3D; \sqrt{\sum_{i&#x3D;1}^{784}(a_i - b_i)^2} $$</p>
<p><strong>直观理解这个距离的含义：</strong><br>这个距离衡量的是两张图片在<strong>逐个像素点上的差异程度</strong>。</p>
<ul>
<li>如果两张图片非常相似（比如两个不同人写的、但都很标准的 “1”），那么它们对应位置的像素值会很接近，<code>(aᵢ - bᵢ)²</code> 的值会很小，最终计算出的总距离也会很小。</li>
<li>如果两张图片差异巨大（比如一个是 “1”，一个是 “8”），那么它们在很多位置上的像素值都会有很大差别，<code>(aᵢ - bᵢ)²</code> 的值会很大，最终计算出的总距离也就会很大。</li>
</ul>
<p><strong>举一个简化的例子：</strong></p>
<p>假设我们处理的是 3x3 的迷你图片，而不是 28x28。</p>
<p><strong>图片A (一个”X”):</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[255,   0, 255],</span><br><span class="line"> [  0, 255,   0],</span><br><span class="line"> [255,   0, 255]]</span><br></pre></td></tr></table></figure>
<p>拉平后的向量 <code>A = (255, 0, 255, 0, 255, 0, 255, 0, 255)</code></p>
<p><strong>图片B (一个”O”):</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[  0, 255,   0],</span><br><span class="line"> [255,   0, 255],</span><br><span class="line"> [  0, 255,   0]]</span><br></pre></td></tr></table></figure>
<p>拉平后的向量 <code>B = (0, 255, 0, 255, 0, 255, 0, 255, 0)</code></p>
<p><strong>计算A和B的欧氏距离：</strong><br>$$ d(A, B) &#x3D; \sqrt{ (255-0)^2 + (0-255)^2 + (255-0)^2 + \dots } $$<br>你会发现每一项都是 <code>(±255)²</code>，这个距离会非常大。</p>
<p>现在，再来一张<strong>图片C (一个略有不同的”X”)</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[250,   5, 250],</span><br><span class="line"> [ 10, 245,  10],</span><br><span class="line"> [250,   5, 250]]</span><br></pre></td></tr></table></figure>
<p>拉平后的向量 <code>C = (250, 5, 250, 10, 245, 10, 250, 5, 250)</code></p>
<p><strong>计算A和C的欧氏距离：</strong><br>$$ d(A, C) &#x3D; \sqrt{ (255-250)^2 + (0-5)^2 + (255-250)^2 + \dots } $$<br>$$ d(A, C) &#x3D; \sqrt{ 5^2 + (-5)^2 + 5^2 + \dots } $$<br>你会发现每一项的差值都很小，所以 <code>d(A, C)</code> 会远远小于 <code>d(A, B)</code>。</p>
<p>因此，在KNN算法中，如果拿图片C去预测，它很可能会找到图片A作为它的近邻，从而被正确地分类为”X”。</p>
<p><strong>面试中的引申问题：</strong></p>
<ol>
<li><p><strong>数据归一化在这里重要吗？</strong></p>
<ul>
<li><strong>回答：</strong> 是的，很重要。虽然MNIST所有特征（像素值）的量纲都是一样的（0-255），但进行归一化（比如将所有像素值除以255，缩放到[0, 1]区间）是一个非常好的习惯。这样做可以<strong>改善模型的数值稳定性</strong>，并且在后续使用梯度下降等优化算法时（虽然KNN不用，但深度学习用）能<strong>加快收敛速度</strong>。所以，这是一个标准的预处理步骤。</li>
</ul>
</li>
<li><p><strong>784维是不是太高了？这有什么问题吗？</strong></p>
<ul>
<li><strong>回答：</strong> 是的，784维属于高维数据。这会引出我们之前讨论过的**“维度灾难”**问题。在高维空间中，数据点会变得稀疏，距离的区分度可能会下降。尽管如此，KNN在MNIST上依然能取得不错的（超过95%）的准确率，证明了这种基于像素的距离度量在一定程度上是有效的。</li>
<li><strong>进一步加分：</strong> 我们可以通过<strong>主成分分析（PCA）<strong>等降维技术，将784维的特征向量降到更低的维度（比如50维或100维），同时保留大部分信息。在降维后的空间里再使用KNN，通常可以</strong>大幅提升计算速度</strong>，并且有时还能<strong>因为去除了噪声而提升模型性能</strong>。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="第三部分：以MNIST数据集为例，说明KNN的执行步骤"><a href="#第三部分：以MNIST数据集为例，说明KNN的执行步骤" class="headerlink" title="第三部分：以MNIST数据集为例，说明KNN的执行步骤"></a><strong>第三部分：以MNIST数据集为例，说明KNN的执行步骤</strong></h3><p><strong>数据集背景:</strong></p>
<ul>
<li><strong>训练集 (Training Set):</strong> 60,000 张 28x28 像素的图片，每张图片都有一个明确的标签（0, 1, 2, …, 9）。这是我们的“参考答案库”。</li>
<li><strong>测试集 (Test Set):</strong> 10,000 张 28x28 像素的图片，同样带有标签。我们用它来评估模型的效果，但在预测时，我们会假装不知道它的标签。</li>
</ul>
<h4 id="KNN在MNIST上的执行步骤"><a href="#KNN在MNIST上的执行步骤" class="headerlink" title="KNN在MNIST上的执行步骤"></a><strong>KNN在MNIST上的执行步骤</strong></h4><h5 id="预备阶段：数据准备-Data-Preparation"><a href="#预备阶段：数据准备-Data-Preparation" class="headerlink" title="预备阶段：数据准备 (Data Preparation)"></a><strong>预备阶段：数据准备 (Data Preparation)</strong></h5><p>在运行算法之前，我们必须先处理数据。</p>
<ol>
<li><p><strong>数据加载：</strong> 从文件中加载60,000张训练图片、对应的60,000个标签，以及10,000张测试图片和对应的10,000个标签。</p>
</li>
<li><p><strong>数据“拉平”(Flattening)：</strong></p>
<ul>
<li>每一张图片是一个 <code>28x28</code> 的二维矩阵。</li>
<li>为了计算距离，我们将其转换为一个 <code>1x784</code> 的一维向量（特征向量）。</li>
<li>操作后，我们的训练集就变成了一个 <code>60000 x 784</code> 的巨大矩阵，每一行代表一张图片。</li>
</ul>
</li>
<li><p><strong>数据归一化 (Normalization)：</strong></p>
<ul>
<li>原始像素值的范围是 [0, 255]。不同图片中笔画的深浅可能会影响距离计算。</li>
<li>我们将所有像素值都除以 255，将它们缩放到 [0, 1] 区间。这是一个标准的预处理步骤，能提升模型的稳定性和性能。</li>
<li>现在，训练集矩阵里的每个元素都在0和1之间。</li>
</ul>
</li>
</ol>
<h5 id="算法执行阶段-以预测一张新的测试图片为例"><a href="#算法执行阶段-以预测一张新的测试图片为例" class="headerlink" title="算法执行阶段 (以预测一张新的测试图片为例)"></a><strong>算法执行阶段 (以预测一张新的测试图片为例)</strong></h5><p>假设我们从10,000张测试图片中，拿出<strong>第一张</strong>图片，想预测它是什么数字。我们称之为<code>test_image_1</code>。</p>
<p><strong>第1步：确定超参数 K</strong><br>这是我们必须预先决定的。K值的选择会影响最终结果。我们先选择一个常见的、较小的值，比如 <strong>K&#x3D;5</strong>。</p>
<p><strong>第2步：计算距离</strong><br>这是最核心、也是最耗时的一步。</p>
<ul>
<li>拿出 <code>test_image_1</code> 的784维向量。</li>
<li>将这个向量与训练集中<strong>每一张</strong>图片（共60,000张）的784维向量，逐一计算<strong>欧氏距离</strong>。</li>
<li>这个过程会产生 <strong>60,000个</strong> 距离值。每一个距离值都代表了 <code>test_image_1</code> 与某一张训练图片的“相似程度”（距离越小越相似）。</li>
</ul>
<p><strong>第3步：找到最近的 K 个邻居</strong></p>
<ul>
<li>我们有了一个包含60,000个距离值的列表。</li>
<li>对这个列表进行<strong>升序排序</strong>，找到值最小的前 K 个。在我们这个例子里，就是找到最小的 <strong>5个</strong> 距离值。</li>
<li>同时，记录下这5个距离值所对应的<strong>训练图片的原始标签</strong>。</li>
</ul>
<p><strong>第4步：投票决定类别</strong></p>
<ul>
<li>假设我们找到的5个最近邻居，它们的标签分别是：<code>&#123;7, 9, 7, 1, 7&#125;</code>。</li>
<li>我们进行“投票统计”：<ul>
<li>数字 “7” 出现了 3 次。</li>
<li>数字 “9” 出现了 1 次。</li>
<li>数字 “1” 出现了 1 次。</li>
</ul>
</li>
<li>根据“少数服从多数”原则，<strong>数字 “7” 的票数最多</strong>。</li>
</ul>
<p><strong>第5步：做出预测</strong></p>
<ul>
<li>我们的KNN模型最终预测 <code>test_image_1</code> 的类别为 <strong>7</strong>。</li>
</ul>
<h5 id="模型评估阶段-Evaluation"><a href="#模型评估阶段-Evaluation" class="headerlink" title="模型评估阶段 (Evaluation)"></a><strong>模型评估阶段 (Evaluation)</strong></h5><p>预测完一张图片还不够，我们需要知道模型的整体性能。</p>
<p><strong>第6步：评估准确率</strong></p>
<ul>
<li>我们拿出 <code>test_image_1</code> 的<strong>真实标签</strong>。假设它的真实标签确实是 “7”，那么这次预测就是<strong>正确</strong>的。如果真实标签是 “2”，那这次预测就是<strong>错误</strong>的。</li>
<li><strong>重复第2步到第5步</strong>，对测试集中<strong>所有10,000张图片</strong>都进行一次预测。</li>
<li>统计所有预测正确的次数。</li>
<li>计算最终的准确率：<br>$$ \text{Accuracy} &#x3D; \frac{\text{Number of Correct Predictions}}{\text{Total Number of Test Images}} &#x3D; \frac{\text{预测正确的图片数}}{10000} $$</li>
</ul>
<p>例如，如果我们正确预测了9,680张图片，那么模型的准确率就是 <code>9680 / 10000 = 96.8%</code>。</p>
<hr>
<h3 id="第四部分：核心考察点"><a href="#第四部分：核心考察点" class="headerlink" title="第四部分：核心考察点"></a><strong>第四部分：核心考察点</strong></h3><h4 id="考点1：KNN的优缺点是什么？"><a href="#考点1：KNN的优缺点是什么？" class="headerlink" title="考点1：KNN的优缺点是什么？"></a><strong>考点1：KNN的优缺点是什么？</strong></h4><p><strong>优点 (Pros):</strong></p>
<ol>
<li><strong>简单直观：</strong> 算法思想简单，易于理解和实现。是很好的入门和基线模型（Baseline）。</li>
<li><strong>非参数模型 (Non-parametric):</strong> 对数据分布没有假设（比如不像线性回归那样假设数据是线性的）。这使得KNN在处理一些非线性、复杂边界问题时有奇效。</li>
<li><strong>无需训练 (Lazy Learning):</strong> KNN是“懒惰学习”的代表。它没有显式的训练过程，只是把训练数据存储起来。训练时间开销为零。</li>
</ol>
<p><strong>缺点 (Cons):</strong></p>
<ol>
<li><strong>计算成本高昂：</strong> 预测阶段的计算量巨大。对于一个新样本，需要和所有N个训练样本计算距离，时间复杂度为O(N*d)，其中d是特征维度。当数据集很大时，预测会非常慢。</li>
<li><strong>内存占用大：</strong> 需要存储全部训练数据，对内存消耗大。</li>
<li><strong>对K值敏感：</strong> K值的选择直接影响模型性能，需要通过交叉验证等方法小心选择。</li>
<li><strong>受样本不均衡问题影响大：</strong> 如果某些类别的样本数量远多于其他类别，那么在投票时，数量多的类别会占据天然优势。</li>
<li><strong>维度灾难 (Curse of Dimensionality)：</strong> 随着特征维度的增加，KNN的性能会急剧下降。</li>
</ol>
<h4 id="考点2：K值的选择与影响-Bias-Variance-Tradeoff"><a href="#考点2：K值的选择与影响-Bias-Variance-Tradeoff" class="headerlink" title="考点2：K值的选择与影响 (Bias-Variance Tradeoff)"></a><strong>考点2：K值的选择与影响 (Bias-Variance Tradeoff)</strong></h4><ul>
<li><p><strong>较小的 K 值：</strong></p>
<ul>
<li>模型会变得更复杂，决策边界会更不规则。</li>
<li>容易受到噪声数据的影响，导致<strong>过拟合 (Overfitting)</strong>。</li>
<li>表现为<strong>低偏差 (Low Bias)</strong> 和 <strong>高方差 (High Variance)</strong>。</li>
</ul>
</li>
<li><p><strong>较大的 K 值：</strong></p>
<ul>
<li>模型会变得更简单，决策边界会更平滑。</li>
<li>会忽略数据中局部的、细微的结构，导致<strong>欠拟合 (Underfitting)</strong>。</li>
<li>表现为<strong>高偏差 (High Bias)</strong> 和 <strong>低方差 (Low Variance)</strong>。</li>
</ul>
</li>
<li><p><strong>如何选择K值？</strong></p>
<ul>
<li><strong>交叉验证 (Cross-Validation):</strong> 最常用的方法。将训练集分成多份，轮流作为验证集来测试不同K值的效果，选择在验证集上平均表现最好的K值。</li>
<li>通常K值会选择一个<strong>奇数</strong>，以避免在二分类问题中出现票数相等的情况。</li>
</ul>
</li>
</ul>
<h4 id="考点3：为什么KNN需要对数据进行归一化-Normalization-Standardization-？"><a href="#考点3：为什么KNN需要对数据进行归一化-Normalization-Standardization-？" class="headerlink" title="考点3：为什么KNN需要对数据进行归一化(Normalization&#x2F;Standardization)？"></a><strong>考点3：为什么KNN需要对数据进行归一化(Normalization&#x2F;Standardization)？</strong></h4><ul>
<li><strong>核心原因：</strong> KNN是基于<strong>距离</strong>的算法。如果不同特征的量纲（尺度）差异巨大，那么距离的计算将完全由尺度大的特征主导。</li>
<li><strong>举例说明：</strong> 假设有两个特征：身高（单位：cm，范围150-200）和体重（单位：kg，范围40-100）。在计算欧氏距离时，身高的差值平方项会比体重的差值平方项大得多，这会使得模型在计算距离时，几乎只考虑了身高，而忽略了体重这个特征。这显然是不合理的。</li>
<li><strong>解决方案：</strong><ul>
<li><strong>归一化 (Normalization):</strong> 将数据缩放到 [0, 1] 区间。公式：<code>X_norm = (X - X_min) / (X_max - X_min)</code></li>
<li><strong>标准化 (Standardization):</strong> 将数据缩放到均值为0，标准差为1的分布。公式：<code>X_std = (X - mean(X)) / std(X)</code></li>
<li>面试时能说出这个原因并给出例子，就是满分回答。</li>
</ul>
</li>
</ul>
<h4 id="考点4：如何理解KNN中的“维度灾难”？"><a href="#考点4：如何理解KNN中的“维度灾难”？" class="headerlink" title="考点4：如何理解KNN中的“维度灾难”？"></a><strong>考点4：如何理解KNN中的“维度灾难”？</strong></h4><ul>
<li><strong>直观理解：</strong> 在高维空间中，数据点会变得非常<strong>稀疏</strong>。想象一下，在一个一维线段上撒10个点很容易密集，但在一个三维立方体里撒10个点，它们会非常分散。</li>
<li><strong>距离失效：</strong> 在高维空间中，任意两点之间的欧氏距离的差值会变得很小，趋向于一致。换句话说，对于一个给定的查询点，它到“最近”邻居的距离和到“最远”邻居的距离相差无几。这使得“近邻”这个概念失去了意义。</li>
<li><strong>如何缓解？</strong><ul>
<li><strong>特征选择&#x2F;降维：</strong> 使用PCA等方法对数据进行降维，去除冗余和不重要的特征。</li>
</ul>
</li>
</ul>
<h4 id="考点5：如何提高KNN的效率？"><a href="#考点5：如何提高KNN的效率？" class="headerlink" title="考点5：如何提高KNN的效率？"></a><strong>考点5：如何提高KNN的效率？</strong></h4><ul>
<li><strong>问题根源：</strong> KNN的瓶颈在于预测时需要进行大量的距离计算（暴力搜索）。</li>
<li><strong>解决方案：</strong> 使用<strong>空间索引数据结构</strong>来加速近邻搜索，避免全局扫描。<ul>
<li><strong>KD-Tree (K-Dimensional Tree):</strong> 这是一种对K维空间中的点进行划分的数据结构。它通过在不同维度上轮流切分数据空间，构建一个二叉树。在搜索时，可以快速剪枝掉那些不可能包含最近邻居的子空间，从而大大减少距离计算的次数。它在<strong>低维</strong>（例如d &lt; 20）时效率很高。</li>
<li><strong>Ball-Tree:</strong> 当维度非常高时，KD-Tree的效率会下降。Ball-Tree通过将数据点划分在一系列嵌套的超球体（Hypersphere）中来构建树。它对高维数据更具鲁棒性。</li>
</ul>
</li>
</ul>
<h4 id="考点6：KNN-和-K-Means-有什么区别？"><a href="#考点6：KNN-和-K-Means-有什么区别？" class="headerlink" title="考点6：KNN 和 K-Means 有什么区别？"></a><strong>考点6：KNN 和 K-Means 有什么区别？</strong></h4><table>
<thead>
<tr>
<th align="left">特性</th>
<th align="left">KNN (K-近邻)</th>
<th align="left">K-Means (K-均值)</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>算法类别</strong></td>
<td align="left"><strong>监督学习</strong> (Supervised Learning)</td>
<td align="left"><strong>无监督学习</strong> (Unsupervised Learning)</td>
</tr>
<tr>
<td align="left"><strong>解决问题</strong></td>
<td align="left"><strong>分类</strong> 或 <strong>回归</strong></td>
<td align="left"><strong>聚类</strong> (Clustering)</td>
</tr>
<tr>
<td align="left"><strong>数据要求</strong></td>
<td align="left">需要带<strong>标签</strong>的训练数据</td>
<td align="left">只需要数据点，<strong>无需标签</strong></td>
</tr>
<tr>
<td align="left"><strong>核心思想</strong></td>
<td align="left">基于邻居投票&#x2F;平均来预测新样本</td>
<td align="left">找到K个簇中心，使数据点到其所属簇中心的距离之和最小</td>
</tr>
<tr>
<td align="left"><strong>“K”的含义</strong></td>
<td align="left">邻居的数量（超参数）</td>
<td align="left">簇的数量（超参数）</td>
</tr>
</tbody></table>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://lx-cel.github.io">秋风、萧瑟</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://lx-cel.github.io/2025/07/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%EF%BC%88KNN%EF%BC%89/">https://lx-cel.github.io/2025/07/05/机器学习—K近邻算法（KNN）/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://lx-cel.github.io" target="_blank">星海流光</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post-share"><div class="social-share" data-image="/img/19.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/07/14/Python%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E5%A4%84%E7%90%86/" title="Python输入输出处理"><img class="cover" src="/img/20.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Python输入输出处理</div></div><div class="info-2"><div class="info-item-1">核心知识点 input() vs sys.stdin.readline()  input(): 内置函数，使用方便，但速度较慢。它会读取一行，去除末尾的换行符 \n，并返回一个字符串。 sys.stdin.readline(): 速度更快，因为它使用了缓冲区。它会读取一行，但保留末尾的换行符 \n。因此，通常需要配合 .strip() 或 .rstrip() 使用。   print() vs sys.stdout.write()  print(): 内置函数，功能强大，可以自动在末尾添加换行符，但相对较慢。 sys.stdout.write(): 速度更快，但只接受字符串作为参数，且不会自动添加换行符，需要手动添加 &#39;\n&#39;。    对于追求极致性能的竞赛，推荐使用 sys 模块。 12import sys# 之后就可以使用 sys.stdin.readline() 和 sys.stdout.write()   一、 输入 (Input)场景1：读取单行数据1.1 读取一个字符串12345678910# 方法一：使用 input() (方便，但稍慢)s =...</div></div></div></a><a class="pagination-related" href="/2025/07/02/Python%E4%B8%AD%E7%9A%84sort%E6%96%B9%E6%B3%95%E3%80%81sorted%E5%87%BD%E6%95%B0%E4%B8%8Elambda%E8%A1%A8%E8%BE%BE%E5%BC%8F/" title="Python中的sort方法、sorted函数与lambda表达式"><img class="cover" src="/img/18.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Python中的sort方法、sorted函数与lambda表达式</div></div><div class="info-2"><div class="info-item-1">1. sort()方法1.1 sort()方法list.sort()最核心、也最需要记住的特点是：它会直接修改原始列表，使其变为有序状态。也就是原地排序。因此，sort()方法的返回值是None。部分初学者可能会将其结果赋值给新变量，这种用法是错误的。正确的用法是无需将其结果赋值给新变量，直接使用被修改后的原列表即可，如下所示： 123nums = [3, 1, 4, 1, 5, 9, 2]nums.sort()print(nums) 结果如下： 1[1, 1, 2, 3, 4, 5, 9] 1.2 基本语法和参数sort()方法的完整语法是： 1list.sort(*, key=None, reverse=False) 它接受两个可自选的关键字参数：key和reverse。 A. reverse参数这个参数非常直观  reverse &#x3D; False（默认值）：升序排序 reverse &#x3D; True：降序排序  123456numbers = [4, 2, 8, 1, 6]numbers.sort() # 默认升序print(numbers) # 输出:...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/25/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E5%88%97%E8%A1%A8%E7%9A%84%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/" title="Python学习笔记1—列表的简单操作"><img class="cover" src="/img/03.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-25</div><div class="info-item-2">Python学习笔记1—列表的简单操作</div></div><div class="info-2"><div class="info-item-1">1. 列表的定义在Pyhton中，用[]来表示列表。并用逗号来分隔其中的元素，在下方是一个简单的列表示例，这个列表包含几种自行车： 12bicycles = [&#x27;trek&#x27;, &#x27;cannondale&#x27;, &#x27;redline&#x27;, &#x27;specialized&#x27;]print(bicycles) 下方为打印结果： 1[&#x27;trek&#x27;, &#x27;cannondale&#x27;, &#x27;redline&#x27;, &#x27;specialized&#x27;] 2. 访问列表元素在Python中，可以通过索引来访问列表元素，如： 12bicycles = [&#x27;trek&#x27;, &#x27;cannondale&#x27;, &#x27;redline&#x27;,...</div></div></div></a><a class="pagination-related" href="/2024/12/26/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02%E2%80%94%E5%88%97%E8%A1%A8%E7%9A%84%E8%BF%9B%E9%98%B6%E6%93%8D%E4%BD%9C/" title="Python学习笔记2—列表的进阶操作"><img class="cover" src="/img/06.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-26</div><div class="info-item-2">Python学习笔记2—列表的进阶操作</div></div><div class="info-2"><div class="info-item-1">1. 使用for循环进行列表遍历假设我们有一个魔术师名单，需要将其中每个魔术师的名字都打印出来，则可以用for 循环进行遍历并打印，如： 123magicians = [&#x27;alice&#x27;, &#x27;david&#x27;, &#x27;carolina&#x27;]for magician in magicians:    print(magician) 这段代码从列表magicians中取出一个名字，并将其存储在变量magician中，进而将其打印，其输出结果如下： 123alice david carolina 2. 创建数值列表2.1 使用函数range()使用函数range()可以生成一些数字，如： 12for value in range(1,5):     print(value) 则打印的结果为： 12341 2 3 4 需要注意的是，range(1,5)只是打印数字1-4，并且range(1,5)并不会生成一个包含数字1-4的列表。 2.2...</div></div></div></a><a class="pagination-related" href="/2024/12/26/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03%E2%80%94%E5%85%83%E7%BB%84/" title="Python学习笔记3—元组"><img class="cover" src="/img/07.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-26</div><div class="info-item-2">Python学习笔记3—元组</div></div><div class="info-2"><div class="info-item-1">1. 元组的创建在Python中元组的定义和用法与列表相似，列表用方括号[]表示，而元组用圆括号()表示，与列表不同的是元组中的元素不可修改，除此之外，Python的列表长度是可变的，而元组长度不可变，元组的定义如下所示： 123dimensions = (200, 50)print(dimensions[0])print(dimensions[1]) 在上面的代码中，先定义了元组dimensions，然后输出索引为0和1的两个元素，其输出结果为： 1220050 如果尝试修改该元组中某个元素的值，则会导致Python报错，如： 12dimensions = (200, 50) dimensions[0] = 250 此时会返回如下错误： 1234Traceback (most recent call last):  File &quot;&lt;pyshell#1&gt;&quot;, line 1, in &lt;module&gt;    dimensions[0] = 250TypeError: &#x27;tuple&#x27; object does not...</div></div></div></a><a class="pagination-related" href="/2024/12/26/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05%E2%80%94%E5%87%BD%E6%95%B0/" title="Python学习笔记5—函数"><img class="cover" src="/img/09.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-26</div><div class="info-item-2">Python学习笔记5—函数</div></div><div class="info-2"><div class="info-item-1">1. 函数的定义在Python中，函数可按下面这种方式定义： 12345def greet_user(username):     &quot;&quot;&quot; 显示简单的问候语 &quot;&quot;&quot;     print(&quot;Hello, &quot; + username.title() + &quot;!&quot;)  greet_user(&#x27;jesse&#x27;) 上面这段代码演示了最简单的函数结构，第一行代码使用关键字def来定义一个函数，def后面的greet_user(username)为函数名，username叫做函数的形参，而’jesse’叫做实参。紧跟在def greet_user(username):后面的所有缩进行构成了函数体。””” 显示简单的问候语 “””叫做文档字符串，描述的函数是做什么的，文档字符串用三引号括起，Python使用它们来生成有关程序中函数的文档。要使用这个函数，可调用它，依次指定函数名以及要传入括号中的参数。上面这段代码的输出为： 1Hello, Jesse! 2....</div></div></div></a><a class="pagination-related" href="/2024/12/26/Python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04%E2%80%94%E5%AD%97%E5%85%B8/" title="Python学习笔记4—字典"><img class="cover" src="/img/08.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-26</div><div class="info-item-2">Python学习笔记4—字典</div></div><div class="info-2"><div class="info-item-1">1. 字典的创建和访问字典中的值在Pyhton中，字典是一系列键值对，每个键都与一个值相关联，可以使用键来访问与之相关联的值。与键相关联的值可以是数字、字符串、列表乃至字典。在Python中，字典用放在花括号{}中的一系列键值对表示，如下面就是一个字典： 1alien_0 = &#123;&#x27;color&#x27;: &#x27;green&#x27;, &#x27;points&#x27;: 5&#125; 键值对是两个相关联的值，指定键时，Python将返回与之相关联的值。键和值之间用冒号分隔，而键值对之间用逗号分隔。 要获取与键相关联的值，可依次指定字典名和放在方括号内的键，如下所示： 12alien_0 = &#123;&#x27;color&#x27;: &#x27;green&#x27;&#125; print(alien_0[&#x27;color&#x27;]) 这将返回字典中alien_0中与键’color’相关联的值： 1green 2....</div></div></div></a><a class="pagination-related" href="/2024/12/26/Python%E6%A0%BC%E5%BC%8F%E8%A7%84%E8%8C%83/" title="Python格式规范"><img class="cover" src="/img/11.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-26</div><div class="info-item-2">Python格式规范</div></div><div class="info-2"><div class="info-item-1">1. Python命名规范1.1 文件名、包名、模块名、变量名、函数名、实例名全部采用小写的形式，并使用下划线连接，如下所示： 12my_car.pymy_car 1.2 类名采用驼峰命名法，即将类中的每个单词的首字母都大写，且不使用下划线，如下所示： 1ElectricCar 1.3 常量名所有字母大写，单词之间采用下划线连接，如下所示： 1MAX_NUM = 100 2. Google Python命名规范 模块名写法: module_name ;包名写法: package_name ;类名: ClassName ;方法名: method_name ;异常名: ExceptionName ;函数名: function_name ;全局常量名: GLOBAL_CONSTANT_NAME ;全局变量名: global_var_name ;实例名: instance_var_name ;函数参数名: function_parameter_name ;局部变量名: local_var_name .  3....</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/Cecilia.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">秋风、萧瑟</div><div class="author-info-description">梦想是用技术创造一个美好的世界</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">35</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">本站于2024年12月25日正式上线,博客内容主要是记录个人学习和技术分享,也欢迎技术交流!!!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%9AKNN%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3-K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95"><span class="toc-text">第一部分：KNN算法详解 (K-近邻算法)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-text">1. 核心思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4-%E4%BB%A5%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E4%B8%BA%E4%BE%8B"><span class="toc-text">2. 算法步骤 (以分类任务为例)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%9A%E4%BB%A5MNIST%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%BA%E4%BE%8B%EF%BC%8C%E8%AF%B4%E6%98%8E%E8%B7%9D%E7%A6%BB%E7%9A%84%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-text">第二部分：以MNIST数据集为例，说明距离的计算过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E7%90%86%E8%A7%A3%E6%95%B0%E6%8D%AE%E6%9C%AC%E8%BA%AB%E2%80%94%E2%80%94%E5%9B%BE%E5%83%8F%E5%A6%82%E4%BD%95%E8%A1%A8%E7%A4%BA%E4%B8%BA%E5%90%91%E9%87%8F"><span class="toc-text">第一步：理解数据本身——图像如何表示为向量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E8%AE%A1%E7%AE%97%E8%B7%9D%E7%A6%BB%E2%80%94%E2%80%94%E5%9C%A8784%E7%BB%B4%E7%A9%BA%E9%97%B4%E4%B8%AD%E4%B8%88%E9%87%8F"><span class="toc-text">第二步：计算距离——在784维空间中丈量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86%EF%BC%9A%E4%BB%A5MNIST%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%BA%E4%BE%8B%EF%BC%8C%E8%AF%B4%E6%98%8EKNN%E7%9A%84%E6%89%A7%E8%A1%8C%E6%AD%A5%E9%AA%A4"><span class="toc-text">第三部分：以MNIST数据集为例，说明KNN的执行步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#KNN%E5%9C%A8MNIST%E4%B8%8A%E7%9A%84%E6%89%A7%E8%A1%8C%E6%AD%A5%E9%AA%A4"><span class="toc-text">KNN在MNIST上的执行步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%A2%84%E5%A4%87%E9%98%B6%E6%AE%B5%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87-Data-Preparation"><span class="toc-text">预备阶段：数据准备 (Data Preparation)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%89%A7%E8%A1%8C%E9%98%B6%E6%AE%B5-%E4%BB%A5%E9%A2%84%E6%B5%8B%E4%B8%80%E5%BC%A0%E6%96%B0%E7%9A%84%E6%B5%8B%E8%AF%95%E5%9B%BE%E7%89%87%E4%B8%BA%E4%BE%8B"><span class="toc-text">算法执行阶段 (以预测一张新的测试图片为例)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E9%98%B6%E6%AE%B5-Evaluation"><span class="toc-text">模型评估阶段 (Evaluation)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%EF%BC%9A%E6%A0%B8%E5%BF%83%E8%80%83%E5%AF%9F%E7%82%B9"><span class="toc-text">第四部分：核心考察点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%80%83%E7%82%B91%EF%BC%9AKNN%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">考点1：KNN的优缺点是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%80%83%E7%82%B92%EF%BC%9AK%E5%80%BC%E7%9A%84%E9%80%89%E6%8B%A9%E4%B8%8E%E5%BD%B1%E5%93%8D-Bias-Variance-Tradeoff"><span class="toc-text">考点2：K值的选择与影响 (Bias-Variance Tradeoff)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%80%83%E7%82%B93%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88KNN%E9%9C%80%E8%A6%81%E5%AF%B9%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%BD%92%E4%B8%80%E5%8C%96-Normalization-Standardization-%EF%BC%9F"><span class="toc-text">考点3：为什么KNN需要对数据进行归一化(Normalization&#x2F;Standardization)？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%80%83%E7%82%B94%EF%BC%9A%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3KNN%E4%B8%AD%E7%9A%84%E2%80%9C%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE%E2%80%9D%EF%BC%9F"><span class="toc-text">考点4：如何理解KNN中的“维度灾难”？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%80%83%E7%82%B95%EF%BC%9A%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98KNN%E7%9A%84%E6%95%88%E7%8E%87%EF%BC%9F"><span class="toc-text">考点5：如何提高KNN的效率？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%80%83%E7%82%B96%EF%BC%9AKNN-%E5%92%8C-K-Means-%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-text">考点6：KNN 和 K-Means 有什么区别？</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/09/19/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94unordered-map/" title="CPP学习笔记——unordered_map"><img src="/img/37.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CPP学习笔记——unordered_map"/></a><div class="content"><a class="title" href="/2025/09/19/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94unordered-map/" title="CPP学习笔记——unordered_map">CPP学习笔记——unordered_map</a><time datetime="2025-09-19T08:09:17.000Z" title="发表于 2025-09-19 16:09:17">2025-09-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/15/%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF/" title="算法模板"><img src="/img/36.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="算法模板"/></a><div class="content"><a class="title" href="/2025/09/15/%E7%AE%97%E6%B3%95%E6%A8%A1%E6%9D%BF/" title="算法模板">算法模板</a><time datetime="2025-09-15T01:13:48.000Z" title="发表于 2025-09-15 09:13:48">2025-09-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/06/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E2%80%94%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84/" title="数据结构与算法—树状数组"><img src="/img/35.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法—树状数组"/></a><div class="content"><a class="title" href="/2025/09/06/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E2%80%94%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84/" title="数据结构与算法—树状数组">数据结构与算法—树状数组</a><time datetime="2025-09-06T02:08:01.000Z" title="发表于 2025-09-06 10:08:01">2025-09-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/29/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/" title="CPP学习笔记—智能指针"><img src="/img/34.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CPP学习笔记—智能指针"/></a><div class="content"><a class="title" href="/2025/08/29/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E6%99%BA%E8%83%BD%E6%8C%87%E9%92%88/" title="CPP学习笔记—智能指针">CPP学习笔记—智能指针</a><time datetime="2025-08-29T01:58:51.000Z" title="发表于 2025-08-29 09:58:51">2025-08-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/28/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E7%89%B9%E6%AE%8A%E6%88%90%E5%91%98%E5%87%BD%E6%95%B0/" title="CPP学习笔记—特殊成员函数"><img src="/img/33.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CPP学习笔记—特殊成员函数"/></a><div class="content"><a class="title" href="/2025/08/28/CPP%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E7%89%B9%E6%AE%8A%E6%88%90%E5%91%98%E5%87%BD%E6%95%B0/" title="CPP学习笔记—特殊成员函数">CPP学习笔记—特殊成员函数</a><time datetime="2025-08-28T06:55:22.000Z" title="发表于 2025-08-28 14:55:22">2025-08-28</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By 秋风、萧瑟</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }
      
      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.isShuoshuo
  const option = null

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'LX-Cel/lx-cel.github.io',
      'data-repo-id': 'R_kgDONd3anw',
      'data-category-id': 'DIC_kwDONd3an84ClwRS',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>